# k8s-course

Source: https://www.linkedin.com/learning/kubernetes-grundkurs

---

### 2025-05-09

#### StorageClass

Parameter der `StorageClass`:
- `Provisioner` legt fest welcher `Provisioner` verwendet werden soll.
- `ReclaimPolicy` legt fest, wie mit nicht mehr gebundene, Speicher umgegangen werden soll.
- `VolumeBindingMode` legt fest wann `Volumes` gebunden werden.
- `AllowVolumeExpansion` legt fest, ob der Speicher eines `Volumes` dynamisch vergr√∂√üert werden darf.

Definition:
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linkedin-storageclass
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false
reclaimPolicy: Delete
```

K√∂nnen wie andere k8s Objekte mit `k apply` ausgerollt werden.
Typische Befehle:

```bash
# Ausrollen
k apply -f <datei.yaml>
# Auflisten
k get sc
```

##### Definition PersistentVolumeClaim

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: linkedin-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: linkedin-storageclass
  resources:
    requests:
      storage: 1Gi
```

Wie jedes andere k8s Objekt mit `k apply` anzuwenden.

Das `Deployment` f√ºr den `PersistentVolumeClaim`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: linkedin-persistent-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: linkedin-persistenz
  template:
    metadata:
      labels:
        app: linkedin-persistenz
    spec:
      containers:
      - name: app
        image: busybox:latest
        command: ["/bin/sh", "-c", "eval echo 'Hello from $(hostname)!' > /data/$(hostname).txt && sleep infinity"]
        volumeMounts:
        - name: linkedin-data-volume
          mountPath: /data
      volumes:
      - name: linkedin-data-volume
        persistentVolumeClaim:
          claimName: linkedin-data-pvc

```

Das `PersistentVolumeClaim` wird von  k8s erst genutzt, wenn dieses von einem `Deployment` verwendet wird.
`k get pv` listet die vorhandenen `PersistentVolumeClaim` auf, der Status `BOUND` gibt an  das es verwendet wird.

`Provisioner`, `StorageClass` und `PersistentVolumeClaim` sind Voraussetzungen f√ºr persistente Datenspeicherung mit Kubernetes.
`PVC` wird in Pod-Definitionen eingebunden und dann per `Volume-Mount` in Container verf√ºgbar gemacht.

`Local-Path-Provisioner` speichert Daten in `/opt/local-path-provisioner` (oder wenn k3s genutzt wird in `/var/lib/rancher/k3s/storage`) ab.
### 2025-05-08
üí≠ es macht Sinn wenn ich mir Literatur Notizen aus meinen Unterlagen erzeuge, bevor ich die ersten Projekte starte. (Pod, Services usw.)
### 2025-05-07 & 2025-05-06

#### Secrets
Funktionieren genau wie `ConfigMaps`.
Informationen k√∂nnen wiederverwendbar verschl√ºsselt verwaltet werden.

- `k get secrets` gibt alle `Secrets` aus.
- `k describe secrets` beschreibt `Secrets` detailliert, die Umgebungsvariablen werden nicht angezeigt, nur deren l√§nge.

#### Definition eines Secrets

```yaml
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: linkedin-secret
data:
  message: "RWluIHZlcnNjaGlzc2VuZXMgR2VoZWltbmlzCg=="
  autor: "RGFuaWVsIE5hZ2VsCg=="
# ---
# Pod welcher Secret als Umgebungsvariable referenziert.
# Schl√ºsselwort envFrom.secretRef f√ºr das referenzieren
# der Secret als Umgebungsvariable.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-secret-env
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "env && sleep 3600"]
      envFrom:
        - secretRef:
            name: linkedin-secret
# ---
# Pod welcher Secret als Datei referenziert.
# Es muss ein Volume angegeben werden, welches das Secret referenziert.
# Mit volumeMounts wird der Pfad angegeben,
# in dem die Secrets im Container gesichert werden soll.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-secrets-file
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "ls /etc/secrets && sleep 3600"]
      volumeMounts:
        - name: secret-volume
          mountPath: /etc/secrets
  volumes:
    - name: secret-volume
      secret:
        secretName: linkedin-secret
```

#### Volumes
https://kubernetes.io/docs/concepts/storage/volumes/
##### Datenaustausch von Pods

Grunds√§tzlich gilt, Daten sind stest an die Instanz gebunden und gehen verloren sobald der Pod gel√∂scht wird.
Werden auf Pod Level definiert.
`Volumes` erlauben den dateisystembasierenden Datenaustausch zwischen Pods.
Dabei gibt es verschiedene Arten von `Volumes`, z.B.:
- [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) ist ein anf√§nglich leeres Verzeichnis, das mit der Zeit neue Daten aufnehmen kann.
- [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) verweist auf ein Verzeichnis auf der Maschine (`local` wird aus Sicherheitsgr√ºnden empfohlen)
- `configMaps` und `Secrets`
`Volumes` teilen sich den Lebenszyklus mit Pods, sobald der letze Pod, mit Zugriff auf das `Volume`, abgeschaltet ist, wird auch das `Volume` freigegeben.
Volumes sind nur f√ºr den kurzen tempor√§ren Datenaustausch gedacht.

Definition:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: linkedin-volume-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: linkedin-volume
  template:
    metadata:
      labels:
        app: linkedin-volume
    spec:
      containers:
      - name: webserver
        image: nginx:alpine
        volumeMounts:
        - name: shared-data
          mountPath: /usr/share/nginx/html
      - name: data-writer
        image: busybox:latest
        command: ["/bin/sh", "-c", "while true; do echo \"It's $(date)\" > /data/index.html; sleep 1; done"]
        volumeMounts:
        - name: shared-data
          mountPath: /data
      volumes:
      - name: shared-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: linkedin-volume-demo
spec:
  selector:
    app: linkedin-volume
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
```

#### Persistent Volumes
https://kubernetes.io/docs/concepts/storage/persistent-volumes/

Daten in Pods sind fl√ºchtig. Eine Persistente Speicherung h√§lt Daten √ºber Pod-Lebenszyklen hinweg verf√ºgbar.
Persistente Speicherorte (lokal, Netzwerk, Cloud) werden mit Hilfe von Volumes abstrahiert.
Volume Mounts machen Volumes in Container verf√ºgbar.

üß† Das Arbeiten mit Kubernetes im Heimnetzwerk hat folgenden Unterschied:
- man Arbeit mit fl√ºchtigen Containern und kann diese einfach verwalten.
- durch die Abstrkation ist das Setup schwerer zu verstehen.
Was ich mir w√ºnsche ist eine einfache, wiederherstellbare, versionierbare Konfiguration.
Allerdings laufen meine Backup System bereits, wof√ºr brauche ich also ein Homelab?
- virtualisieren
- datenaustausch
- backups

`Persistent Volumes` Definition und Lebenszyklus sind unabh√§ngig vom Pod.
Dadurch erlauben sie eine dauerhafte Datenspeicherung.

Es gibt weitere Arten von Komponente, welche k8s zur Datenspeicherung bereit stellt:
- `Provisioner`: stellt Speicher bereit..
- `StorageClass`: vordefinierte Konfiguration https://kubernetes.io/docs/concepts/storage/storage-classes/
- `PersistenVolume`: repr√§sentiert einen Speicherort.
- `PersistentVolumeClaim`: Anfrage, Speicher bereit zu stellen.

##### local-path-provisioner

Bereitstellen:
`kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.31/deploy/local-path-storage.yaml`

Der `local-path-provisioner` ist die grunlegende Komponente um mit persistentem Speicher in Kubernetes arbeiten zu k√∂nnen.
### 2025-05-05 & 2025-05-06

#### Secrets
Funktionieren genau wie `ConfigMaps`.
Informationen k√∂nnen wiederverwendbar verschl√ºsselt verwaltet werden.

- `k get secrets` gibt alle `Secrets` aus.
- `k describe secrets` beschreibt `Secrets` detailliert, die Umgebungsvariablen werden nicht angezeigt, nur deren l√§nge.

#### Definition eines Secrets

```yaml
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: linkedin-secret
data:
  message: "RWluIHZlcnNjaGlzc2VuZXMgR2VoZWltbmlzCg=="
  autor: "RGFuaWVsIE5hZ2VsCg=="
# ---
# Pod welcher Secret als Umgebungsvariable referenziert.
# Schl√ºsselwort envFrom.secretRef f√ºr das referenzieren
# der Secret als Umgebungsvariable.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-secret-env
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "env && sleep 3600"]
      envFrom:
        - secretRef:
            name: linkedin-secret
# ---
# Pod welcher Secret als Datei referenziert.
# Es muss ein Volume angegeben werden, welches das Secret referenziert.
# Mit volumeMounts wird der Pfad angegeben,
# in dem die Secrets im Container gesichert werden soll.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-secrets-file
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "ls /etc/secrets && sleep 3600"]
      volumeMounts:
        - name: secret-volume
          mountPath: /etc/secrets
  volumes:
    - name: secret-volume
      secret:
        secretName: linkedin-secret
```

#### ConfigMaps
https://kubernetes.io/docs/concepts/configuration/configmap/
`ConfigMaps` sind √ºber Pods und `Deployments` hinweg wiederverwendbare _Umgebungsvariablen_ bzw. externalisiert und wiederverwendbar Konfigurationen.
Diese werden in Pod-Definitionen abgefragt und k√∂nnen mehrfach verwendet werden.
Bei `ConfigMaps` handelt es sich um `Kubernetes` Objekte welche mit `kubetcl apply` bereit gestellt werden k√∂nnen.

`ConfigMaps` k√∂nnen als Umgebungsvariable oder als Datei eingebunden werden.
Alle Eintr√§ge stehen dann als einzelne Dateien zur Verf√ºgung (Dateiname = Name des `ConfigMap`-Parameters).
√Ñnderungen erfordern einen Neustart der nutzenden Pods!
Die Art der Einbindung wird nicht √ºber die `ConfigMap` sondern den verwendenden Pod gesteuert.
Die Einbindung erfolgt Statisch, d.h. beim Start des Dateisystems.

##### Definition einer ConfigMap

```yaml
# ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: linkedin-configmap
data:
  message: "Hallo von Kubernetes!"
  autor: "Daniel Nagel"
  KURS: "Kubernetes Grundkurs"
  KURS_URL: "https://www.linkedin.com/learning/kubernetes-grundkurs"
# ---
# Pod welcher ConfigMap als Umgebungsvariable referenziert.
# Schl√ºsselwort envFrom.configMapRef f√ºr das referenzieren
# der ConfigMap als Umgebungsvariable.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-configmap-env
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "env && sleep 3600"]
      envFrom:
        - configMapRef:
            name: linkedin-configmap

# ---
# Pod welcher ConfigMap als Datei referenziert.
# Es muss ein Volume angegeben werden, welches die ConfigMap referenziert.
# Mit volumeMounts wird der Pfad angegeben,
# an dem die ConfigMap im Container gesichert werden soll.
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-configmap-file
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "ls /etc/config && sleep 3600"]
      volumeMounts:
        - name: config-volume
          mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: linkedin-configmap
```

- Mit `envFrom.configMapRef.name` kann ein Pod eine `ConfigMap` referenzieren.
- Mit `k get configmaps` k√∂nnen alle `ConfigMaps` angezeigt werden.
- Mit `k describe configmaps <configmap>` k√∂nnen die Details der `ConfigMap` eingesehen werden.
- Mit `k exec -it linkedin-confimap-file -- cat /etc/config/KURS` einen Befehl in einem Pod ausf√ºhren, in diesem Beispiel wird der Inhalt einer Datei ausgegeben.
### 2025-05-02

#### Unver√§nderliche Applikationen

Container-Images werden nur einmal gebaut, daher werden alle Konfigurationen extern gehalten (das gilt vor allem f√ºr Kennw√∂rter und sensible Daten), das Prinzip hei√üt **externalisierte Konfiguration**.
Die Informationen werden erst zur Laufzeit zur Verf√ºgung gestellt.

##### Arten von externalisierte Konfiguration

- `Umgebungsvariablen`
- `ConfigMaps`
	- √ºber Pods und Deployments hinweg wiederverwendbare _Umgebungsvariablen_.
	- https://kubernetes.io/docs/concepts/configuration/configmap/
	- Es sollten keine sensiblen Daten gespeichert werden.
- `Secrets`
	- vertraulich Informationen (Kennw√∂rter, Tokens, etc.) f√ºr Pods und Deployments sichern.
	- https://kubernetes.io/docs/concepts/configuration/secret/
- `Volumes`
	- Daten auf Festplatten oder in virtuellen Dateisystemen sichern.
	- https://kubernetes.io/docs/concepts/storage/volumes/
		- Eine Konfigurationsdatei, basierend auf ConfigMaps oder Secrets, im Datei System sichern.
		- Ein Dateisystem zwischen zwei Containern im selben Pod einrichten.
		- Ein Dateisystem zwischen zwei verschiedenen Pods, sogar auf unterschiedlichen Nodes, einrichten.
		- Daten √ºber die Laufzeit eines Pods hinaus sichern.

##### Umgebungsvariablen verwenden

Beispiel YAML-Konfiguration:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-env-demo
spec:
  containers:
    - name: busybox
      image: busybox:latest
      command: ["/bin/sh", "-c", "env && sleep 3600"]
      env:
        - name: NACHRICHT
          value: "Hallo von Kubernetes!"
```

### 2025-04-29 & 2025-04-30 & 2025-05-02

#### Ingress
https://kubernetes.io/docs/concepts/services-networking/ingress/
Ein `Ingress` ist eine spezielle Variante eines `Services`.
Zugriff nur via HTTP/HTTPS. Durch einen regelbasierten Ansatz werden weitere `Services` an den `Ingress` gebunden.
Es wird eine Load-Balancing Funktionalit√§t angebunden und Ingress kann SSL Verbindungen Terminieren, wodurch diese unverschl√ºsselt innerhalb des Services weiterlaufen, was performanter ist.

![[regelbasierte-ingress-weiterleitung.png]]
Ein `Ingress` kann Anfragen zu einem bestimmten Pfad an einen `Service` weiterleiten, dieser spricht wiederum seinen entsprechenden `Pod` an.
Regeln k√∂nnen Pfade oder Domains oder eine Kombination aus beidem sein.
Die jeweilige Ingress Komponente muss installiert sein.
Kubernetes Ingress Controller sind austauschbar.

##### Ingress-Definition
In YAML spezifziert

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: linkedin-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /foo
        pathType: Prefix
        backend:
          service:
            name: service-a
            port:
              number: 80
```

##### nginx-Ingress ausrollen
https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file
Am einfachsten √ºber ein `HELM`-Chart.

```bash
# nginx repo installieren
helm repo add nginx https://kubernetes.github.io/ingress-nginx
# alle repos updaten
helm repo update
# suche nach nginx in allen repositories
helm search repo nginx
# installieren/upgrade des nginx/ingress-nginx Packets,
# im namespace ingress-nginx, welcher automatisch angelegt wird
helm upgrade --install ingress-nginx nginx/ingress-nginx --namespace ingress-nginx --create-namespace
# auflisten aller namespaces
k get ns
# auflisten aller pods im ingress-nginx namensraum
k get po -n ingress-nginx
```

##### traefik ingress ausrollen
https://doc.traefik.io/traefik/getting-started/install-traefik/#use-the-helm-chart
`Traefik` ist auch ein beliebter Ingress Controller und zeichnet sich durch eine besonders einfache Konfiguration und gut Performance aus. `Traeffik` wir wie der `nginx`-Ingress am einfachsten per `HELM`-Chart ausgerollt.

```bash
helm repo add traefik https://traefik.github.io/charts
helm repo update
helm search repo traefik
helm upgrade --install traefik traefik/traefik --namespace ingress-traefik --create-namepsace
```

#### Ingress definieren
Es muss zuvor ein `Deployment` bereitgestellt und ein `Service` ausgerollt werden. Dann kann ein `Ingress` mit mindestens einer Regel definiert werden.

Mein Ingress h√§ngt in einem Pending state fest, wie es aussieht muss ich Metallb auf einem lokalen (nicht Cloud) Umgebung installieren:
https://medium.com/@galvao.gabrielg/metallb-the-solution-for-external-ips-in-kubernetes-with-loadbalancer-learn-more-23fc2aff4e2b

Wenn ich es lokal Testen m√∂chte geht es doch ich musste einfach √ºber `localhost:<ingress-port>/test` zugreifen.

Immerhin habe ich jetzt eine offizielle Dokumentation zu k3s mit tailscale gefunden: https://docs.k3s.io/networking/distributed-multicloud
### 2025-04-28 & 2025-04-29

#### Services
https://kubernetes.io/docs/concepts/services-networking/service/
`Pods/Deployments` sind volatil und kurzlebig sind, k√∂nnen skaliert und terminiert werden, werden im Fehlerfall neu gestartet werden, wobei eine neue IP vergeben wird und sind somit fl√ºchtige Komponenten.
Ein `Service` repr√§sentiert eine Menge an Pods, hat eine eindeutige und stabile IP, bietet Load-Balancer funktionalit√§t und ist somit eine stabile Komponente.

`Pods` exponieren Labels und `Services` definieren f√ºr welche Services sie zust√§ndig sind.
Beziehung zwischen `Pod` und `Service` wird durch Labels definiert.
![[fuktionsweise-von-k8s-services.png]]
Ein Service wird wie folgt definiert:

```yaml
# Service
apiVersion: v1
kind: Service
metadata:
  name: service-a
spec:
  type: ClusterIP
  selector:
    # label f√ºr pod-service kommunikation
    tier: service-a
  ports:
  - protocol: TCP
    port: 8000
    # pod port
    targetPort: 80

# Zum Service zugeh√∂riger Pod
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    # label f√ºr pod-service kommunikation
    tier: service-a
spec:
  containers:
  - name: nginx
    image: nginx:stable
    ports:
    # pod port
    - containerPort: 80
      name: http-web-svc
```

##### Typen von Services
https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
- `ClusterIP` ein Service mit einer nur im Cluster g√ºltigen IP-Adresse (am h√§ufigsten verwendeter Service-Typ)
- `NodePort` Services sind √ºber alle Nodes im Cluster verf√ºgbar.
- `LoadBalancer` Services werden √ºber einen externen Load Balancer bereit gestellt und k√∂nnen somit √ºber eine virtuelle oder externe IP-Adresse angesprochen werden.
- `ExternalName` Services machen externe Komponenten im Cluster verf√ºgbar.

##### Prinzip
Ein `Pod` definiert einen Labels. Ein `Service` nutzt Labels, um zu repr√§sentierende `Pods`zu identifizieren. Eingehende Anfragen werden dann per Load-Balancer auf die repr√§sentierten `Pods` verteilt.

##### Ausrollen eines Services
Ein `Deployment` **muss** vorher oder danach ausgerollt werden.
Die Reihenfolge, ob `Pods` oder `Service` zuerst ausgerollt werden spielt keine Rolle.
```bash
# Deployment im Cluster ausrollen
k apply -f configurations/deployment.yaml
# Service im Cluster ausrollen
k apply -f configurations/service.yaml
# Auflisten der Services
k get svc
# Beziehung zwischen Service und Pod betrachten
k describe svc nginx-service
```

### 2025-04-27

#### HELM
https://helm.sh/
`HELM` ist ein **package Manager**, dieser vereinfacht Installation, Upgrade und Verwaltung von Anwendungen. Anbieter k√∂nnen Repositories definieren, um dort ihre `Charts` abzulegen.
`Charts` sind YAML-basierte Beschreibungen von Kubernetes-Ressourcen.
Mit `Charts` lassen sich komplexe Applikationen bereitstellen, eine gro√üe Menge an vorgefertigten `Charts` ist verf√ºgbar.
`HELM` erleichtert die Bereitstellung und Veraltung von Applikationen, organisiert `Charts` in Repositories, Werte von `Charts` k√∂nnen beim Rollout oder bei Updates angepasst werden und es ist ein sehr gerne genutztes Tool f√ºr k8s.

Beim `HELM` muss man zwischen drei Konzepten Unterscheiden:
- Ein `Chart` ist ein Helm Package. Es beinhaltet alle n√∂tigen Definitionen um eine Applikation, Werkzeug oder Dienst im Kubernetes Cluster auszuf√ºhren.
- Ein `Repository` sammelt und verteilt `Charts` an einem Ort. So wie bei Pacakge Mangern, z.B. `apt` oder `npm`.
- Ein `Release` ist eine Instanz eines laufenden `Charts` innerhalb eines Kubernetes-Clusters. Ein `Chart` kann mehrfach im selben Cluster installiert werden, was wiederum neue `Releases`erzeugt.

Kurz gesagt: `Helm` installiert `Charts` in ein Kubernets-Cluster, welches neue `Releases` f√ºr jede Installation erzeugt. Neue Charts k√∂nnen in einem installierten Helm Chart `Repository` gesucht werden.

##### HELM installieren
https://helm.sh/docs/intro/quickstart/

##### Wichtige HELM-Befehle
- `helm repo add` f√ºgt ein Repository hinzu.
- `helm repo update` l√§dt die Meta-Informationen von Repositories.
- `helm search repo` durchsucht Repositories.
- `helm install` installiert eine Applikation.
- `helm update` aktualisiert eine Applikation.
- `helm delete` l√∂scht eine Applikation.
- `helm list` Listet die installierten Applikationen auf.
Parameter k√∂nnen inline mit `--set` √ºberschrieben werden.

##### Applikation mit HELM bereitstellen
Alle Wordpress Parameter gibt es hier: https://github.com/bitnami/charts/blob/main/bitnami/wordpress/README.md#parameters
Kann auch mit `helm show values bitnami/wordpress` aufgelistet werden.

```bash
# repository installieren
# dieses Repository wird auch vom HELM Quickstart empfohlen
helm repo add bitnami https://charts.bitnami.com/bitnami
# repository aktualisieren
helm repo update
# Alle Pakete im Repository auflisten
helm search repo
# Nach einem Paket im Repository suchen
helm search repo wordpress
# Paket installieren, f√ºr Parameter des wordpress Pakets, siehe obigen Link.
helm install wordpress bitnami/wordpress --set mariadb.auth.rootPassword=sicher123 --set ingress.enabled=false --set service.type=NodePort --set readinessProbe.enabled=false --set livenessProbe.enabled=false
# auflisten der k8s Services, so k√∂nnen wir sehen,
# unter welchem Port der Webserver Wordpress ausliefert.
k get svc
# Listet die installierten Applikationen auf
helm list
# Zeigt die eingestellten, vom Standard abweichenden, Parameter
helm get values wordpress
# Deinstalliert das Release, alle Kubernetes-Ressourcen werden freigegeben
helm uninstall wordpress
```

### 2025-04-20 & 2025-04-21 & 2025-04-26

#### Deployments
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
##### Nachteile von Replicasets

Replicatsets bieten grundlegende Funktionen rund um Skalierung und unterbrechungsfreie Updates, allerdings k√∂nnen Replicastes nicht erkenne, wie der Status eines Pods ist, ob ein Pod einsatzbereit ist und k√∂nnen Pods nicht auf √§ltere St√§nde zur√ºckrollen.

##### Vorteile von Deployments

Deployments bauen auf Replicasets auf und bieten erweiterte Funktionalit√§ten rund um Gesundheitschecks, Update-Strategien und Rollbacks an.
Deployments werden von k8s verwaltet und in der Regel im YAML-Format definiert.

Definition eines Deployments:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
```
##### Deployment ausrollen
Per CLI kann, unter der notwendigen Angabe des Deployment Namens und des Container-Images, mit dem Befehl `kubectl create deployment` ein Deployment ausgerollt werden.
Diese Art des Deployment ist aufgrund der nicht existenten Versionierung nicht empfehlenswert.

Die Eigenschaften `livenessProbe`und `readinessProbe` lassen sich nur in Deployments definieren.
`livenessProbe` legen fest, ob ein Pod noch erreichbar ist/ funktioniert.
`readinessProbe` stellt sicher ob ein Pod initial verf√ºgbar ist.

Eine YAML-Defintion eines Deployments kann wie gehabt mit `kubectl apply -f <config.yaml>` gestartet werden.

##### Deployment skalieren
###### Manuell
Ein Deployment kann deklarativ durch die Modifikation der YAML-Datei oder direkt √ºber die Kommandozeile skaliert werden.
Dabei wir die Anzahl der Pods unmittelbar ge√§ndert.
Dazu wird in der YAML-Datei die `replicas` Eigenschaft angepasst oder tempor√§r per CLI mit dem Befehl `kubectl scale deployment`.
###### Automatisch
https://kubernetes.io/docs/concepts/workloads/autoscaling/
https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/
Ein Deployment kann automatisch basierend auf Metriken, z.B. CPU-Auslastung, Speicherverbrauche, benutzerdefinierte Metriken, die Anzahl der Pods anpassen. Die Mindest- und Maximalanzahl der Pods und die Schwellwerte f√ºr die Skalierung sind konfigurierbar.
Das automatische Skalieren von Deployments erfolgt per `HorizontalPodAutoscaler` (beim horizontalen skalieren, siehe [auoscaling](https://kubernetes.io/docs/concepts/workloads/autoscaling/)). Das ist entweder deklarativ per YAML-Datei ausrollbar oder einfacher √ºber die Kommandozeile m√∂glich `kubectl autoscale deployment nginx --max=5 --min=1 --cpu-percent=80`.

Beispiel YAML-Konfiguration:
```yaml
# equivalent zum CLI Befehl
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
```

##### Deployment l√∂schen
Mit dem Befehl `kubectl delete deployment` werden Deployments gel√∂scht. Alle zugeh√∂rigen Pods und Ressourcen (Replicasets) werden ebenfalls gel√∂scht. Eine Skalierung auf 0 vor dem L√∂schen ist optional, aber Empfohlen. Mit der Options `--cascade=false` wird das L√∂schen abh√§ngiger Ressourcen verhindert., mit `--grace-perid=<Sekunden>` wird eine Karenzzeit vor dem L√∂schen festgelegt.

##### Gesundheitscheck f√ºr Deployments
Eine Liveness Probe pr√ºft, ob der Container l√§uft und reagiert. Es gibt diverse Arten von Probes: HTTP-Anfragen, TCP.-Socket-Verbindungen, Ausf√ºhren von Befehlen im Container. Die Pr√ºfung kann mit verschiedenen Parametern konfiguriert werden: H√§ufigkeit (`periodSeconds`), Fehlerschwelle (`failureThreshold`) und Wartezeit (`initialDelaySeconds`).
F√ºr professionelle Applikationen und Workloads sind `livenessProbes` unbedingt notwendig.

##### Verf√ºgbarkeit von Deployments pr√ºfen
`readinessProbe` pr√ºft ob ein Container bereit ist, Anfragen zu verarbeiten. Bei einem Fehlschlage wird der Pod neu gestartet. Dabei gibt es verschieden Arten von √úberpr√ºfungen (HTTP-Anfragen, TCP-Socket-Verbindungen, Ausf√ºhren von Befehlen im Container) und fein granulare Konfigurationen: H√§ufigkeit (`periodSeconds`), Fehlerschwelle (`failureThreshold`) und Wartezeit (`initialDelaySeconds`) der √úberpr√ºfungen.
`livenessProbe`und `readinessProbe` werden h√§ufig zusammen eingesetzt.

##### StatefulSets
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
Ein zustand behaftetes Deployment.
Wird √ºblicherweise bei Datenbanken, verteilten Systemen oder Anwendungen die stabile Netzwerkidentit√§ten oder persistenten Speicher ben√∂tigen eingesetzt. Ein `StatefulSet` garantiert die geordnete Erstellung, Skalierung und L√∂schung von Pods. Jeder Pod hat eine eindeutige Identit√§t und einen stabilen Netzwerknamen. `StatefulSet` unterst√ºtzt einen persistenten Speicher f√ºr jeden Pod.

Es m√ºssen Abh√§ngigkeiten definiert sein, `StorageClass` etc.
Definition und Bereitstellung sind analog zu "normalen" Deployments, durch die m√∂glichen Konfigurationen sind `StatfulSet` besonders f√ºr Datenbanken oder √§hnliches geeignet.

Beispielkonfiguration:

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
spec:
  serviceName: mongodb
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: mongo:latest
          ports:
            - containerPort: 27017
              name: mongo
          volumeMounts:
            - name: mongodb-data
              mountPath: /data/db
  volumeClaimTemplates:
    - metadata:
        name: mongodb-data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: local-path
        resources:
          requests:
            storage: 1Gi
```

##### DaemonSets
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
Deployments auf allen Nodes.
Werden √ºblicherweisen f√ºr Logging, Monitorong, Netzwerk-Plug-Ins oder Cluster-Agenten, d.h. die √ºberall laufen sollen, eingesetzt.
Diese laufen auf jedem Node in einem Cluster, durch Node-Affinit√§ten k√∂nnen diese aber an bestimmte Nodes gebunden werden.
Ein `DaemonSet` erstellt automatisch neue Pods auf neuen Nodes und l√∂scht sie auf entfernten Nodes.
Grunds√§tzlich gilt: **alle Pods eines `DaemonSet` laufen auf allen Nodes.**

##### Jobs
https://kubernetes.io/docs/concepts/workloads/controllers/job/
Aufgaben bezogene Jobs.
Werden meist genutzt um Aufgaben einmalig auszuf√ºhren, die au√üerhalb eines normalen Kontextes stattfinden, also Batch-Verarbeitungen, Datenanalyse oder einmalige Aufgaben.
Jobs erstellen Pods f√ºr genau eine Aufgabe und beenden den Pod nach der Ausf√ºhrung wieder, ohne diese erneut zu starten, falls ein Fehler aufgetreten ist.

##### CronJobs
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
Zeit gesteuerte Ausf√ºhrung von Jobs.
Werden f√ºr wiederkehrende Aufgaben genutzt, z.B. Back-ups, Berichte, Datenbereinigung und andere regelm√§√üige Aufgaben.
`CronJobs` erstellen `Jobs` zu festgelegten Zeiten oder Intervallen und die Konfiguration basiert auf dem Linux-Cron-Format f√ºr Zeitplanungen.
Zus√§tzlich bieten `CronJobs` Wiederholungen und Fehlerbehandlungen bei fehlschlagenden Jobs an.

### 2025-04-19

#### Replicasets
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
Pods verf√ºgen √ºber keinen Mechanismus zur Skalierung.
Wenn Pods ausfallen oder gel√∂scht werden, werden sie nicht automatisch wiederhergestellt.
Pods bieten keine Funktionalit√§ten rund um unterbrechungsfreie (Zero-Downtime) Updates.

Replicasets stellen sicher, dass immer die gew√ºnschte Anzahl an Pods l√§uft und sie bieten Funktionalit√§ten wie Skalierung und unterbrechungsfreie Updates an.
Replicasets werden √ºber K8s, in aller Regel im Yaml-Format definiert, verwaltet.

Definition eines Replicasets:
```yaml
apiVersion: v1
kind: Replicaset
metadata:
	name: linkedin-replicaset
spec:
	selector:
		matchLabels:
			app: linkedin-pod
template:
	metadata:
		labels:
			app: linkedin-pod
	spec:
		containers:
		- name: linkedin-container
		  image: busybox
# ...
```
Das Replicaset sucht nach einer genauen √úbereinstimmung zwischen `spec/selector/matchLabels` und `template/metadata/labels`
#### Labels und Annotationen
Labels und Annotationen definieren:
```yaml
apiVersion: v1
kind: Pod
metadata:
	name: linkedin-pod
	labels:
		app: linkedin
		environment: prod
	annotations:
		author: "Daniel Nagel"
		version: "1.0.0"
spec:
	# ...
```
##### Labels
https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
Labels sind Schl√ºssel/Wert-Paare. Die Schl√ºssel m√ºssen eindeutig sein und d√ºrfen maximal 63 Zeichen lang sein. Die Schl√ºssel k√∂nnen f√ºr Filterung/Auswahl genutzt werden.
Werden √ºblicherweise genutzt um Kubernetes-Objekte zu finden (m√∂gliche Operatoren: `=`, `!=`, `in`, `notin`).
Das erm√∂glicht die flexible Zuordnung von Kubernetes-Objekten zueinander, was Labels essentiell f√ºr die Automatisierung und das Verwalten von Clustern macht.

Die Labels einer Node k√∂nnen festlegen ob ein Pod auf einem bestimmten Node laufen darf oder nicht:
```yaml
apiVersion: v1
kind: Pod
metadata:
	name: linkedin-pod
	labels:
		app: linkedin
		environment: prod
	nodeSelector:
		disktype: ssd
		architecture: arm
spec:
	# ...
```
##### Annotationen
https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
Annotationen sind Schl√ºssel/Wert-Paare. Die Schl√ºssell√§nge ist unlimitiert und kann nicht zur Filterung/Auswahl genutzt werden. Es handelt sich bei Annotationen um reine Meta-Informationen.
#### Namensr√§ume
https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
Namensr√§ume (`namespaces`) trennen und organisieren Ressourcen f√ºr isolierte Umgebungen (Entwicklung, Test, Produktion).
Es gibt vordefinierte Standard-Namensr√§ume, welche mit `kube` beginnen, die in Ruhe gelassen werden sollten.
Eine √úbersicht aller Namensr√§ume erh√§lt man mit `kubectl get namespaces`.
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace
Einen neuen Namensraum erstellt man mit `kubectl create namespace`.
Auflisten der Pods in einem Namensraum mit `kubectl get pod -n kube-system`.
Grunds√§tzlich gilt, alle `kubectl` Befehle k√∂nnen auf einem bestimmten Namensraum ausgef√ºhrt werden `kubectl ... -n <namespace>`.

`kubectl` administriert nur die Pods im `default` Namensraum, wenn kein Namesraum angegeben wurde.
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace
Mit `kubectl delete namespace` kann ein gesamter Namensraum mit allen Ressourcen gel√∂scht werden. Dieser Vorgang kann etwas Zeit in Anspruch nehmen.

Der Standard Namensraum kann in der Konfiguration festgelegt werden. Dies erfolgt mit dem Befehl `kubectl config set-context --current --namespace default`.
#### Node-Affinit√§ten
https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors
Node-Affinit√§ten sind Regeln f√ºr die Pod-Platzierung auf Nodes. Basieren auf dem Vorhandesein oder der Abwesenheit bestimmter Labels auf Nodes.
Definieren weiche ("bevorzugte", `nodeAffinity`) und harte ("ausschlie√ülich" `nodeSelector`) Regeln.
Node-Affinit√§ten erzwingen √úbereinstimmungen mit Node-Labels.
Erst bei einer √úbereinstimmung wir der Pod ausgerollt.
Label zu Node hinzuf√ºgen: `kubectl label node`
##### nodeSelector
Einfache Definition durch harte Regeln, da eine genau √úbereinstimmung n√∂tig ist.
##### nodeAffinity
Gr√∂√üere Flexibilit√§t, da harte und weiche Regeln durch Listen und Ausschlusskirterien m√∂glich sind (`In`, `NotIn`, `Exists`, ...)

Node-Affinit√§ten kommen in echten Clustern in Frage, in Single-Node-Clustern macht das wenig Sinn. Das ist zwar gut zu Wissen, aber f√ºr meine F√§lle erstmal irrelevant.

### 2025-04-18

#### Sidecars

https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/

Ein zus√§tzlicher Container welcher mit dem Haupt-Container zusammen in einem Pod l√§uft.
Dieser bietet zus√§tzlich Funktionen f√ºr den Haupt-Container ohne diesen zu ver√§ndern. Haupt- und Sidecar-Container teilen sich die Ressourcen, etwa Netzwerk oder Speicher.
Sidecars sind zus√§tzliche Eintr√§ge in der Konfiguration, so wie `initContainers`.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: linkedin-sidecar
spec:
  containers:
    - name: linkedin-container
      image: busybox:latest
      command: ["sh", "-c", "while true; do echo $(date) >> /data/time.txt; sleep 5; done"]
      volumeMounts:
        - name: data
          mountPath: /data
    - image: busybox:latest
      name: sidecar
      command: ["sh", "-c", "tail -f /data/time.txt"]
      volumeMounts:
        - name: data
          mountPath: /data
  volumes:
    - name: data
      emptyDir: {}
```

Ein Sidecar hat den selben Lebenszyklus wie der Hauptcontainer, d.h. der Sidecar Container wird mit dem Hauptcontainer gestartet und beendet.

Mir ist aufgefallen das Konfiguration nicht erneut angewendet werden k√∂nnen (`apply`), der betroffene `Pod` muss zuvor gel√∂scht werden.

Bei der Verwaltung von Sidecars ist zu beachten, dass diese bei der Verwaltung explizit angegeben werden m√ºssen.

```bash
# Logs des Haupt-Containers
k logs linkedin-sidecar
# Logs des Sidecars (Der Name des Containers ist sidecar)
k logs linkedin-sidecar -c sidecar
```

#### Requests und Limits

Requests und Limits definieren minimal und maximale CPI- und Speichervorgaben. Diese werden von K8s zur Zuordnung von Pods zu Nodes genutzt. Die Angabe sch√ºtzt vor √ºberm√§√üiger Nutzung und Engp√§ssen und stellt die Cluster-Stabilit√§t sicher.

Requests und Limits sind extrem wichtig, nur so kann k8s die Cluster-Stabilit√§t sicherstellen.
Produktiv m√ºssen Requests und Limits vorhanden sein, sonst k√∂nnen Applikationen nicht nachhaltig betrieben werden!
##### Requests

Requests definieren Untergrenzen, d.h. minimale Ressourcenanforderungen der Pods, somit wird die Verf√ºgbarkeit sichergestellt. K8s nutzt diese Informationen um sicherzustellen auf welchen Nodes die Pods laufen k√∂nnen und ob diese √ºberhaupt noch laufen k√∂nnen (Scheduling und Pod-Zuordnung).

##### Limits

Limits definieren Obergrenzen, d.h. diese werden dazu verwendet damit K8s Pods √ºberwachen kann. Diese legen eine maximale Ressourcenbegrenzung fest, vermeiden √úbernutzung und sorgen daf√ºr das K8s die Pods bei √úberschreitung beendet. Die Cluster-Stabilit√§t hat immer Vorrang gegen√ºber der Gesundheit einzelner Pods.

#### Busybox Image

Das¬†**BusyBox**-Image ist ein leichtgewichtiges, minimalistisches Linux-Image, das eine Sammlung von Unix-Utilities in einer einzigen ausf√ºhrbaren Datei kombiniert. Es wird h√§ufig in Container-Umgebungen verwendet, da es extrem klein ist (wenige Megabyte) und dennoch grundlegende Funktionen f√ºr Shell-Skripte und Systemadministration bereitstellt.

BusyBox enth√§lt viele grundlegende Unix-Utilities wie¬†`ls`,¬†`cp`,¬†`mv`,¬†`cat`,¬†`echo`,¬†`grep`,¬†`vi`,¬†`wget`¬†und viele mehr. Es wird h√§ufig als Basis-Image f√ºr Container verwendet, die keine vollst√§ndige Linux-Distribution ben√∂tigen. 

Es k√∂nnte verwendet werden, um einfache Aufgaben wie das Ausf√ºhren von Shell-Skripten, Debugging oder das Bereitstellen von Testumgebungen zu erledigen.
Zum Beispiel k√∂nnte es f√ºr einen Sidecar-Container verwendet werden, der Logs sammelt oder einfache Netzwerkoperationen ausf√ºhrt.
### 2025-04-17
#### Verbindung mit Pods
Verbindung zu Pods ist m√∂glich, aber √Ñnderungen betreffen stets eine Pod Instanz.
Somit sollten nur Lesezugriffe durchgef√ºhrt werden.
Mit `kubectl exec` kann Code im Container ausgef√ºhrt werden.
Beispiele:
```bash
#  Auflisten des working dirs
kubectl exec linkedin-timer-pod -- ls -la
# Shell im Pod √∂ffnen
kubectl exec linkedin-timer-pod -it -- /bin/sh
```

#### Init-Container

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

Spezielle Variante eines Container im Kubernetes-Umfeld, welcher genutzt wird um den eigentlichen Pod zu initialisieren.
Typische Anwendungsf√§lle sind: Datenbank-Updates, Set-up von Komponenten, √úberpr√ºfen von Stammdaten, Updates, etc.
Der eigentliche Container/Pod wird erst nach erfolgreicher Ausf√ºhrung des Init-Containers gestartet.
Init-Container werden nur bei erneutem Deployment oder bei ge√§ndertem Init-Container-Image erneut ausgef√ºhrt.
Sind mehrere Init-Container definierte, werden diese in der Reihenfolge ihrere Definition ausgef√ºhrt.
Init-Container werden im Init-Container Bereich der YAML definiert.
Beispiel Konfiguration:

```yaml
apiVersion: v1
kind: Pod
metadata:
    name: linkedin-pod
spec:
    initContainers:
    - name: init-container
      image: busybox
      command: ['sh', '-c', 'echo Init Container! && sleep 10']
    containers:
    - name: linkedin-container
      image: busybox
      command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 1800']
```
### 2025-04-16

#### korrekte kubectl Berechtigung

Quelle:  https://0to1.nl/post/k3s-kubectl-permission/

`k3s` Konfiguration in die User Konfiguration kopieren und Pfad zu dieser in der `KUBECONFIG` Umgebungsvariable setzen.

```bash
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config && chown $USER ~/.kube/config && chmod 600 ~/.kube/config && export KUBECONFIG=~/.kube/config
```

Anschlie√üend den `export` in der `.zshrc` sichern.
#### Pod

https://kubernetes.io/docs/concepts/workloads/pods/
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

In Kubernetes k√∂nnen Container nicht direkt bereitgestellt werden. Stattdessen erfolgt die Ausf√ºhrung √ºber sogenannte **Pods**, die die kleinste und zugleich atomare Deployment-Einheit innerhalb von Kubernetes darstellen. Ein Pod repr√§sentiert damit ein Container-Deployment und beinhaltet nicht nur einen oder mehrere Container, sondern auch s√§mtliche Meta-Informationen, die f√ºr deren Ausf√ºhrung notwendig sind.

Die Beschreibung eines Pods erfolgt in der Regel √ºber eine **Manifest-Datei im YAML-Format**, in der der gew√ºnschte Zustand des Deployments definiert wird. Administratoren spezifizieren in dieser Datei, wie die Container im Pod aussehen sollen, welche Ressourcen sie ben√∂tigen und wie sie sich verhalten sollen ‚Äì Kubernetes sorgt anschlie√üend daf√ºr, dass dieser gew√ºnschte Zustand eingehalten wird.

Als **Orchestrator** √ºbernimmt Kubernetes dabei vielf√§ltige Aufgaben: Es entscheidet, auf welchem Knoten ein Pod ausgerollt wird, √ºberwacht kontinuierlich die System- und Pod-Gesundheit, beendet nicht funktionierende Pods und startet sie bei Bedarf neu. Ziel ist es, den gew√ºnschten Zustand der Umgebung konstant sicherzustellen ‚Äì ganz ohne manuelles Eingreifen.

**Pods** selbst verf√ºgen √ºber Mechanismen zur Statusberichterstattung und k√∂nnen Metriken sowie Logs exponieren, um eine √úberwachung und Analyse zu erm√∂glichen. Da alle internen Informationen beim Beenden eines Pods verloren gehen, m√ºssen wichtige Daten stets extern gespeichert werden, zum Beispiel √ºber Persistent Volumes.

Zusammenfassend l√§sst sich sagen: Kubernetes √ºbernimmt das komplette Management von Pods. Administratoren m√ºssen lediglich definieren, **was** erreicht werden soll ‚Äì **wie** dieses Ziel erreicht wird, regelt Kubernetes automatisch.
##### Roll-out Optionen
###### Direktes Deployment
```bash
sudo k3s kubectl run linkedin-pod-cmd --image busybox -- /bin/sh -c "echo 'Hello Kubernetes' && sleep 1800"
```
###### Konfiguration per YAML Datei
```yaml
apiVersion: v1
kind: Pod
metadata:
    name: linkedin-pod
spec:
    containers:
    - name: linkedin-container
      image: busybox
      command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 1800']

```
Anwenden der Konfiguration:
```bash
sudo k3s kubectl apply -f pod.yaml
```
Da die YAML-Datei versioniert werden kann, ist diese Deployment zu bevorzugen.
##### Verwaltung
`kubectl` erlaubt die Verwaltung von Pods, die Verwaltungs-Operationen sind `get`, `describe`, `edit` und `apply`.

```bash
# Mehr Informationen zu den Pods erhalten
sudo k3s kubectl get pod -o wide
# Kurzschreibweise von pod
sudo k3s kubectl get po
kgp
# zum beobachten --watch Option nutzen
kgp --watch
# Status eines Pods im yaml Format
sudo k3s kubectl get pod linkedin-pod -o yaml
# Besser lesbarer Status des Pods
sudo k3s kubectl describe pod linkedin-pod
# Editieren der Metadaten eines Pods
sudo k3s kubectl edit pod linkedin-pod
```
Mit der `edit` Operation kann lediglich die Container Version angepasst werden.
Das anpassen der yaml-Konfiguration und dem erneuten `apply` hat den selben Effekt.
##### L√∂schen
Nach dem L√∂schen werden s√§mtlich Ressourcen freigegeben, auch Dateien und Logs.
Das l√∂schen ist eine synchrone Aktion und dauert daher einen Moment.
```bash
# L√∂schen eines Pods
sudo k3s kubectl delete pod linkedin-pod-cmd
```
##### Logs einsehen
Logs von Pods sind extern Abrufbar, dabei sind alle Standard- und Fehlerausgaben eines Pods verf√ºgbar und das speichern der Logs in Dateien ist nicht notwendig.
Abruf √ºber `kubectl logs <podname>`.

Mit `-f` k√∂nnen Logs beobachtet werden.
Beispiel yaml-Konfiguration:
```yaml
apiVersion: v1
kind: Pod
metadata:
    name: linkedin-timer-pod
spec:
    containers:
    - name: linkedin-timer-container
      image: busybox:latest
      command: ['sh', '-c', 'for i in $(seq 1 1800); do echo $(date) && sleep 1; done']
```
Logt 30 Minuten jeden Sekunde das aktuelle Datum.
### 2025-04-15

##### Single Node Kubernetes
- Kubernetes auf nur einer Maschine
	- f√ºr gew√∂hnlich wird kubernetes auf mehreren Maschinen gleichzeitig betrieben
- ControlPlane-Node kann Workloads ausf√ºhren
	- kontrolliert den Status des Clusters, die kontrollierten Nodes (workloads) liegen au√üerhalb
- es gibt verschiedene Single Node Kubernetes Distributionen
	- k3s, Minikube, MicroK8s, selbst installiert

Habe jetzt [k3s](https://k3s.io/) installiert, in der Hoffnung das ich damit dem Kurs folgen kann.

- `kubectl` wird ben√∂tigt um mit einem Cluster zu interagieren.
- Versionen von `kubectl` und Cluster sollten √ºbereinstimmen.
- Konfiguration f√ºr Cluster befindet sich in `~/.kube`